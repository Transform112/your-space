{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.805896805896806,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09828009828009827,
      "grad_norm": 45.20686721801758,
      "learning_rate": 0.00018,
      "loss": 3.2969,
      "step": 10
    },
    {
      "epoch": 0.19656019656019655,
      "grad_norm": 8.487692832946777,
      "learning_rate": 0.00019995921928281894,
      "loss": 1.7554,
      "step": 20
    },
    {
      "epoch": 0.29484029484029484,
      "grad_norm": 8.063132286071777,
      "learning_rate": 0.00019981829160444514,
      "loss": 1.4505,
      "step": 30
    },
    {
      "epoch": 0.3931203931203931,
      "grad_norm": 7.214399814605713,
      "learning_rate": 0.00019957685536765995,
      "loss": 1.244,
      "step": 40
    },
    {
      "epoch": 0.4914004914004914,
      "grad_norm": 7.437548637390137,
      "learning_rate": 0.0001992351536782881,
      "loss": 1.2271,
      "step": 50
    },
    {
      "epoch": 0.5896805896805897,
      "grad_norm": 6.137208938598633,
      "learning_rate": 0.00019879353060096603,
      "loss": 1.1589,
      "step": 60
    },
    {
      "epoch": 0.687960687960688,
      "grad_norm": 6.332849502563477,
      "learning_rate": 0.00019825243081269774,
      "loss": 1.1893,
      "step": 70
    },
    {
      "epoch": 0.7862407862407862,
      "grad_norm": 6.845974922180176,
      "learning_rate": 0.00019761239915510302,
      "loss": 1.1351,
      "step": 80
    },
    {
      "epoch": 0.8845208845208845,
      "grad_norm": 6.635626792907715,
      "learning_rate": 0.00019687408008580784,
      "loss": 1.0864,
      "step": 90
    },
    {
      "epoch": 0.9828009828009828,
      "grad_norm": 5.986627101898193,
      "learning_rate": 0.00019603821702953046,
      "loss": 1.0835,
      "step": 100
    },
    {
      "epoch": 1.0786240786240786,
      "grad_norm": 6.523216247558594,
      "learning_rate": 0.00019510565162951537,
      "loss": 1.0112,
      "step": 110
    },
    {
      "epoch": 1.1769041769041768,
      "grad_norm": 6.486752510070801,
      "learning_rate": 0.00019407732290007023,
      "loss": 0.9399,
      "step": 120
    },
    {
      "epoch": 1.2751842751842752,
      "grad_norm": 6.581074237823486,
      "learning_rate": 0.00019295426628105792,
      "loss": 0.9329,
      "step": 130
    },
    {
      "epoch": 1.3734643734643734,
      "grad_norm": 6.826773166656494,
      "learning_rate": 0.00019173761259529633,
      "loss": 0.9508,
      "step": 140
    },
    {
      "epoch": 1.4717444717444716,
      "grad_norm": 6.472663879394531,
      "learning_rate": 0.00019042858690991574,
      "loss": 0.9556,
      "step": 150
    },
    {
      "epoch": 1.57002457002457,
      "grad_norm": 6.575468063354492,
      "learning_rate": 0.00018902850730281992,
      "loss": 0.9595,
      "step": 160
    },
    {
      "epoch": 1.6683046683046683,
      "grad_norm": 7.6202826499938965,
      "learning_rate": 0.00018753878353549357,
      "loss": 0.9051,
      "step": 170
    },
    {
      "epoch": 1.7665847665847667,
      "grad_norm": 6.663663864135742,
      "learning_rate": 0.00018596091563349192,
      "loss": 0.9608,
      "step": 180
    },
    {
      "epoch": 1.864864864864865,
      "grad_norm": 7.349499225616455,
      "learning_rate": 0.00018429649237604217,
      "loss": 0.9262,
      "step": 190
    },
    {
      "epoch": 1.9631449631449631,
      "grad_norm": 6.9224114418029785,
      "learning_rate": 0.0001825471896962774,
      "loss": 0.9422,
      "step": 200
    },
    {
      "epoch": 2.058968058968059,
      "grad_norm": 6.263022422790527,
      "learning_rate": 0.00018071476899371414,
      "loss": 0.8094,
      "step": 210
    },
    {
      "epoch": 2.157248157248157,
      "grad_norm": 8.898387908935547,
      "learning_rate": 0.00017880107536067218,
      "loss": 0.7518,
      "step": 220
    },
    {
      "epoch": 2.2555282555282554,
      "grad_norm": 10.384270668029785,
      "learning_rate": 0.00017680803572442318,
      "loss": 0.8055,
      "step": 230
    },
    {
      "epoch": 2.3538083538083536,
      "grad_norm": 6.83204460144043,
      "learning_rate": 0.0001747376569069381,
      "loss": 0.8153,
      "step": 240
    },
    {
      "epoch": 2.4520884520884523,
      "grad_norm": 7.492844104766846,
      "learning_rate": 0.00017259202360418762,
      "loss": 0.8021,
      "step": 250
    },
    {
      "epoch": 2.5503685503685505,
      "grad_norm": 9.960766792297363,
      "learning_rate": 0.00017037329628703004,
      "loss": 0.7991,
      "step": 260
    },
    {
      "epoch": 2.6486486486486487,
      "grad_norm": 6.915140151977539,
      "learning_rate": 0.00016808370902580036,
      "loss": 0.7749,
      "step": 270
    },
    {
      "epoch": 2.746928746928747,
      "grad_norm": 7.532264232635498,
      "learning_rate": 0.00016572556724079056,
      "loss": 0.7692,
      "step": 280
    },
    {
      "epoch": 2.845208845208845,
      "grad_norm": 8.355978012084961,
      "learning_rate": 0.00016330124538088705,
      "loss": 0.7963,
      "step": 290
    },
    {
      "epoch": 2.9434889434889433,
      "grad_norm": 9.129166603088379,
      "learning_rate": 0.0001608131845327018,
      "loss": 0.8295,
      "step": 300
    },
    {
      "epoch": 3.039312039312039,
      "grad_norm": 7.344890117645264,
      "learning_rate": 0.00015826388996260503,
      "loss": 0.7342,
      "step": 310
    },
    {
      "epoch": 3.1375921375921374,
      "grad_norm": 8.901280403137207,
      "learning_rate": 0.0001556559285941344,
      "loss": 0.6274,
      "step": 320
    },
    {
      "epoch": 3.235872235872236,
      "grad_norm": 8.629364967346191,
      "learning_rate": 0.0001529919264233205,
      "loss": 0.6594,
      "step": 330
    },
    {
      "epoch": 3.3341523341523343,
      "grad_norm": 7.238920211791992,
      "learning_rate": 0.0001502745658745316,
      "loss": 0.6282,
      "step": 340
    },
    {
      "epoch": 3.4324324324324325,
      "grad_norm": 7.342770099639893,
      "learning_rate": 0.0001475065830994995,
      "loss": 0.6668,
      "step": 350
    },
    {
      "epoch": 3.5307125307125307,
      "grad_norm": 8.886783599853516,
      "learning_rate": 0.0001446907652222468,
      "loss": 0.6377,
      "step": 360
    },
    {
      "epoch": 3.628992628992629,
      "grad_norm": 8.572993278503418,
      "learning_rate": 0.00014182994753268927,
      "loss": 0.703,
      "step": 370
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 8.435903549194336,
      "learning_rate": 0.00013892701063173918,
      "loss": 0.6833,
      "step": 380
    },
    {
      "epoch": 3.8255528255528253,
      "grad_norm": 9.5535249710083,
      "learning_rate": 0.00013598487753078425,
      "loss": 0.6412,
      "step": 390
    },
    {
      "epoch": 3.923832923832924,
      "grad_norm": 8.747708320617676,
      "learning_rate": 0.00013300651070846333,
      "loss": 0.686,
      "step": 400
    },
    {
      "epoch": 4.019656019656019,
      "grad_norm": 7.084843635559082,
      "learning_rate": 0.00012999490912770107,
      "loss": 0.6258,
      "step": 410
    },
    {
      "epoch": 4.117936117936118,
      "grad_norm": 8.417520523071289,
      "learning_rate": 0.0001269531052160068,
      "loss": 0.5164,
      "step": 420
    },
    {
      "epoch": 4.216216216216216,
      "grad_norm": 10.281911849975586,
      "learning_rate": 0.0001238841618120769,
      "loss": 0.5721,
      "step": 430
    },
    {
      "epoch": 4.314496314496314,
      "grad_norm": 8.572307586669922,
      "learning_rate": 0.00012079116908177593,
      "loss": 0.5251,
      "step": 440
    },
    {
      "epoch": 4.412776412776413,
      "grad_norm": 9.077258110046387,
      "learning_rate": 0.00011767724140660157,
      "loss": 0.5363,
      "step": 450
    },
    {
      "epoch": 4.511056511056511,
      "grad_norm": 7.738473415374756,
      "learning_rate": 0.00011454551424776637,
      "loss": 0.5585,
      "step": 460
    },
    {
      "epoch": 4.6093366093366095,
      "grad_norm": 12.574627876281738,
      "learning_rate": 0.00011139914098905406,
      "loss": 0.5493,
      "step": 470
    },
    {
      "epoch": 4.707616707616707,
      "grad_norm": 9.76574993133545,
      "learning_rate": 0.00010824128976162964,
      "loss": 0.5491,
      "step": 480
    },
    {
      "epoch": 4.805896805896806,
      "grad_norm": 8.8334321975708,
      "learning_rate": 0.00010507514025399943,
      "loss": 0.554,
      "step": 490
    },
    {
      "epoch": 4.9041769041769046,
      "grad_norm": 10.919758796691895,
      "learning_rate": 0.00010190388051033466,
      "loss": 0.5426,
      "step": 500
    },
    {
      "epoch": 5.0,
      "grad_norm": 14.631999969482422,
      "learning_rate": 9.873070372038105e-05,
      "loss": 0.551,
      "step": 510
    },
    {
      "epoch": 5.098280098280099,
      "grad_norm": 9.749176979064941,
      "learning_rate": 9.55588050041874e-05,
      "loss": 0.4486,
      "step": 520
    },
    {
      "epoch": 5.196560196560196,
      "grad_norm": 10.492070198059082,
      "learning_rate": 9.239137819489047e-05,
      "loss": 0.4642,
      "step": 530
    },
    {
      "epoch": 5.294840294840295,
      "grad_norm": 8.565433502197266,
      "learning_rate": 8.92316126227961e-05,
      "loss": 0.4247,
      "step": 540
    },
    {
      "epoch": 5.393120393120393,
      "grad_norm": 9.378211975097656,
      "learning_rate": 8.608268990399349e-05,
      "loss": 0.4488,
      "step": 550
    },
    {
      "epoch": 5.4914004914004915,
      "grad_norm": 9.282776832580566,
      "learning_rate": 8.294778073673762e-05,
      "loss": 0.447,
      "step": 560
    },
    {
      "epoch": 5.58968058968059,
      "grad_norm": 9.61748218536377,
      "learning_rate": 7.983004170882518e-05,
      "loss": 0.4447,
      "step": 570
    },
    {
      "epoch": 5.687960687960688,
      "grad_norm": 9.850689888000488,
      "learning_rate": 7.673261211917776e-05,
      "loss": 0.4566,
      "step": 580
    },
    {
      "epoch": 5.7862407862407865,
      "grad_norm": 9.746150970458984,
      "learning_rate": 7.365861081683433e-05,
      "loss": 0.4425,
      "step": 590
    },
    {
      "epoch": 5.884520884520884,
      "grad_norm": 12.68909740447998,
      "learning_rate": 7.061113306053443e-05,
      "loss": 0.4656,
      "step": 600
    },
    {
      "epoch": 5.982800982800983,
      "grad_norm": 10.40776252746582,
      "learning_rate": 6.759324740205495e-05,
      "loss": 0.4441,
      "step": 610
    },
    {
      "epoch": 6.078624078624078,
      "grad_norm": 7.865938663482666,
      "learning_rate": 6.460799259643884e-05,
      "loss": 0.372,
      "step": 620
    },
    {
      "epoch": 6.176904176904177,
      "grad_norm": 9.840497016906738,
      "learning_rate": 6.165837454222608e-05,
      "loss": 0.3658,
      "step": 630
    },
    {
      "epoch": 6.275184275184275,
      "grad_norm": 11.421542167663574,
      "learning_rate": 5.8747363254768894e-05,
      "loss": 0.3692,
      "step": 640
    },
    {
      "epoch": 6.3734643734643734,
      "grad_norm": 9.379626274108887,
      "learning_rate": 5.5877889875677845e-05,
      "loss": 0.3638,
      "step": 650
    },
    {
      "epoch": 6.471744471744472,
      "grad_norm": 10.640320777893066,
      "learning_rate": 5.305284372141095e-05,
      "loss": 0.3805,
      "step": 660
    },
    {
      "epoch": 6.57002457002457,
      "grad_norm": 13.372865676879883,
      "learning_rate": 5.027506937397652e-05,
      "loss": 0.3693,
      "step": 670
    },
    {
      "epoch": 6.6683046683046685,
      "grad_norm": 10.135412216186523,
      "learning_rate": 4.754736381668057e-05,
      "loss": 0.3701,
      "step": 680
    },
    {
      "epoch": 6.766584766584766,
      "grad_norm": 8.75672721862793,
      "learning_rate": 4.487247361780169e-05,
      "loss": 0.3694,
      "step": 690
    },
    {
      "epoch": 6.864864864864865,
      "grad_norm": 11.419885635375977,
      "learning_rate": 4.225309216502933e-05,
      "loss": 0.3781,
      "step": 700
    },
    {
      "epoch": 6.963144963144963,
      "grad_norm": 12.577223777770996,
      "learning_rate": 3.969185695345105e-05,
      "loss": 0.3636,
      "step": 710
    },
    {
      "epoch": 7.058968058968059,
      "grad_norm": 10.487329483032227,
      "learning_rate": 3.719134692981826e-05,
      "loss": 0.3415,
      "step": 720
    },
    {
      "epoch": 7.157248157248158,
      "grad_norm": 10.426114082336426,
      "learning_rate": 3.47540798957656e-05,
      "loss": 0.3189,
      "step": 730
    },
    {
      "epoch": 7.255528255528255,
      "grad_norm": 7.376840591430664,
      "learning_rate": 3.238250997259808e-05,
      "loss": 0.3168,
      "step": 740
    },
    {
      "epoch": 7.353808353808354,
      "grad_norm": 11.901637077331543,
      "learning_rate": 3.0079025130198935e-05,
      "loss": 0.3195,
      "step": 750
    },
    {
      "epoch": 7.452088452088452,
      "grad_norm": 8.293062210083008,
      "learning_rate": 2.7845944782546453e-05,
      "loss": 0.3351,
      "step": 760
    },
    {
      "epoch": 7.5503685503685505,
      "grad_norm": 22.689912796020508,
      "learning_rate": 2.5685517452260567e-05,
      "loss": 0.3292,
      "step": 770
    },
    {
      "epoch": 7.648648648648649,
      "grad_norm": 8.991971969604492,
      "learning_rate": 2.3599918506531337e-05,
      "loss": 0.3265,
      "step": 780
    },
    {
      "epoch": 7.746928746928747,
      "grad_norm": 10.00607967376709,
      "learning_rate": 2.159124796670843e-05,
      "loss": 0.3122,
      "step": 790
    },
    {
      "epoch": 7.8452088452088455,
      "grad_norm": 9.267878532409668,
      "learning_rate": 1.9661528393757744e-05,
      "loss": 0.3143,
      "step": 800
    },
    {
      "epoch": 7.943488943488943,
      "grad_norm": 11.084664344787598,
      "learning_rate": 1.7812702851713904e-05,
      "loss": 0.3295,
      "step": 810
    },
    {
      "epoch": 8.039312039312039,
      "grad_norm": 8.67514419555664,
      "learning_rate": 1.6046632951179508e-05,
      "loss": 0.3127,
      "step": 820
    },
    {
      "epoch": 8.137592137592138,
      "grad_norm": 7.930743217468262,
      "learning_rate": 1.4365096974841108e-05,
      "loss": 0.2912,
      "step": 830
    },
    {
      "epoch": 8.235872235872236,
      "grad_norm": 8.053122520446777,
      "learning_rate": 1.2769788086889134e-05,
      "loss": 0.2898,
      "step": 840
    },
    {
      "epoch": 8.334152334152334,
      "grad_norm": 7.9688496589660645,
      "learning_rate": 1.126231262814521e-05,
      "loss": 0.2932,
      "step": 850
    },
    {
      "epoch": 8.432432432432432,
      "grad_norm": 8.020854949951172,
      "learning_rate": 9.844188498613116e-06,
      "loss": 0.2879,
      "step": 860
    },
    {
      "epoch": 8.530712530712531,
      "grad_norm": 11.487314224243164,
      "learning_rate": 8.516843629081984e-06,
      "loss": 0.279,
      "step": 870
    },
    {
      "epoch": 8.628992628992629,
      "grad_norm": 9.850893020629883,
      "learning_rate": 7.281614543321269e-06,
      "loss": 0.3011,
      "step": 880
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 9.037299156188965,
      "learning_rate": 6.139745012314424e-06,
      "loss": 0.2938,
      "step": 890
    },
    {
      "epoch": 8.825552825552826,
      "grad_norm": 8.433796882629395,
      "learning_rate": 5.092384801887074e-06,
      "loss": 0.2899,
      "step": 900
    },
    {
      "epoch": 8.923832923832924,
      "grad_norm": 8.175762176513672,
      "learning_rate": 4.140588514990162e-06,
      "loss": 0.2872,
      "step": 910
    },
    {
      "epoch": 9.01965601965602,
      "grad_norm": 13.949938774108887,
      "learning_rate": 3.2853145298042953e-06,
      "loss": 0.2852,
      "step": 920
    },
    {
      "epoch": 9.117936117936118,
      "grad_norm": 13.264586448669434,
      "learning_rate": 2.5274240347340717e-06,
      "loss": 0.2825,
      "step": 930
    },
    {
      "epoch": 9.216216216216216,
      "grad_norm": 7.773394584655762,
      "learning_rate": 1.8676801612643957e-06,
      "loss": 0.2793,
      "step": 940
    },
    {
      "epoch": 9.314496314496315,
      "grad_norm": 8.167527198791504,
      "learning_rate": 1.3067472155517735e-06,
      "loss": 0.271,
      "step": 950
    },
    {
      "epoch": 9.412776412776413,
      "grad_norm": 8.678779602050781,
      "learning_rate": 8.451900095242881e-07,
      "loss": 0.2795,
      "step": 960
    },
    {
      "epoch": 9.51105651105651,
      "grad_norm": 7.002770900726318,
      "learning_rate": 4.834732921638719e-07,
      "loss": 0.2776,
      "step": 970
    },
    {
      "epoch": 9.609336609336609,
      "grad_norm": 7.794518947601318,
      "learning_rate": 2.219612815434924e-07,
      "loss": 0.2769,
      "step": 980
    },
    {
      "epoch": 9.707616707616708,
      "grad_norm": 8.419631004333496,
      "learning_rate": 6.09172980904238e-08,
      "loss": 0.2772,
      "step": 990
    },
    {
      "epoch": 9.805896805896806,
      "grad_norm": 7.584167003631592,
      "learning_rate": 5.034994448926967e-10,
      "loss": 0.2733,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 657348706080000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
